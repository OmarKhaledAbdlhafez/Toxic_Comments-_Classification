# Toxic_Comments_Classification
## working with  [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) kaggle Competition which target Detect toxicity across a diverse range of conversations
### my main goal was to apply different approach to learn more and dive in more into nlp models  , the dataset was so imbalance so i use undersampling method to override this problem 
- the frist & second notebook i apply the same thing , i try to using classic representation of text like countvector & TFIDF and use any machine learning model to do classification and the results was very good even i don't do any pr-processing for data check the code on kaggle [here](https://www.kaggle.com/code/omarkhald/01-classic-nlp-solution) and [here](https://www.kaggle.com/code/omarkhald/02-classic-nlp-solution)

- in [03- simple word Embeddings solution](https://www.kaggle.com/code/omarkhald/03-simple-word-embeddings-solution)  in this notebook i try to use the concept of embedding which is very good representation of text so i use spacy which have nlp method that can give u the embedding vector for the sentence it get the vector of each word and then get the average of all words in the sentence and give u the sentence vector and then use an ANN model to classification  

- in [04- word Embeddings & ngrams using cnn] (https://www.kaggle.com/code/omarkhald/04-word-embeddings-ngrams-using-cnn) in this notebook i try to use embedding for each word so each sample will be with shape of (bacth , maxlength of sentence , word vector )  and then use cnn with different filter which will work as n-gram , The window size indicate the N in N-gram, like for example if the kernel size is 3x300, this means 3-grams model and so on

- in [05- word Embeddings & LSTM] (https://www.kaggle.com/code/omarkhald/05-word-embeddings-lstm) in this notebook i use the most famous nlp  approach in deep learning which use embedding layer and any of (rnn , lstm , gru ) layer 
- in [06- use transformer and simple NN](https://www.kaggle.com/code/omarkhald/06-use-transformer-and-simple-nn) in this notebook i use  the transfer learning concept from sentence-transformers This framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity , so i use this model to compute the sentences vector and then apply my classification task using simple ANN .
## the best accuracy i get is 86% with .79 score on kaggle leaderborad  
